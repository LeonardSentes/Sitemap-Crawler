# Sitemap-Crawler

## Introduction

A sitemap crawler is a tool or program that is designed to automatically scan and crawl through the pages of a website, using the URLs provided in its sitemap file. The file will contain a list of URLs on a website that the e owner wants search engines to index. By using this, website owners can ensure that all the pages listed in their sitemap file are accessible and properly linked, and that there are no broken links or errors that could negatively affect their search engine rankings. Sitemap crawlers can be used for many different purposes, such as checking for broken links or duplicate content, identifying pages that are not properly linked, and ensuring that all pages are properly indexed by search engines. 

## Purpose
The purpose of this project is to develop a Python-based application that can scan a website's links and search for broken links, corrupted photos, and videos. This application will help website owners to identify and fix errors on their websites, thereby improving their search engine rankings and user experience.

# Trello Board:
To keep track of the project's progress, we will use Trello. You can find the link to the Trello board here: https://trello.com/b/xVDekw9w/sitemap-crawler-activity

# Installation
To install and use the sitemap crawler application, please follow these steps:

1. Clone the repository: git clone [https://github.com/username/sitemap-crawler.git](https://github.com/Sitemap-Crawler/Sitemap-Crawler.git)
2. Install the dependencies: pip install -r requirements.txt
3. Run the application: python app.py

## Docker Option

Alternatively, you can use Docker to run the sitemap crawler application. Here are the instructions to do so:

1. Install Docker on your machine: https://www.docker.com/products/docker-desktop
2. Open a terminal window and navigate to the project directory.
3. Build the Docker image and run the docker container: `docker-compose up`

# Usage:
To use the sitemap crawler application, please follow these steps:

1. Enter the URL of the website you want to scan.
2. The application will scan the website and report any broken links, corrupted photos, or videos.
3. The application will also generate a report of the website's sitemap, highlighting any pages that are not properly linked or indexed.

# License
This project is licensed under the MIT License.

# Contributing
Contributions to this project are welcome. Please open an issue or pull request if you find any bugs or have suggestions for improvements.
